Assignment 1 [80 Points]
In this assignment, we will learn how to parallelize simple programs using C++11 thread library - std::thread (note: this is different from pthreads). There are four problems in this assignment, and for each problem you are provided the serial C++ implementation, the expected parallelization strategy, and the expected output to be generated by your parallel solution.

Before starting this assignment, you should have completed Tutorial 1 which walks you through how to use our servers for your code development.

Performance of a program is often sensitive to the way in which data is laid out in memory. Tutorial 2 explains how data layout in memory can affect performance. You should ensure your solutions do not suffer from false sharing, and are not limited by poor choice of data layout.

General Instructions
You are provided with the serial version of all the programs here. To run a program (e.g., pi_calculation.cpp), follow the steps below:
Run make pi_calculation. This creates a binary file called pi_calculation.
Create a slurm job to run the binary file using the following command: ./pi_calculation --nPoints 12345678
Use the command-line argument --nPoints to specify the number of points to be generated (detailed description about pi_calculation mentioned below).
All parallel programs should have the command-line argument --nWorkers to specify the number of threads for the program. Example: --nWorkers 4.
While testing your solutions, make sure that cpus-per-task is correctly specified in your slurm config file based on your requirement.
You will be asked to print the time spent by different threads on specific code regions. The time spent by any code region can be computed as follows:
timer t1;
t1.start();
/* ---- Code region whose time is to be measured --- */
double time_taken = t1.stop();
Sample outputs for all the programs can be found in sample_outputs directory. Programs will be evaluated and graded automatically. Please make sure that your program output strictly follows the sample output format.
We have provided test scripts for you to quickly test your solutions during your development process. You can test your code using the test script available at /scratch/assignment1/test_scripts/. Note that these test scripts only validate the output formats, and a different evaluation script will be used for grading the assignments. Important: You should use slurm when performing these and other tests. The test scripts under /scratch/assignment1/test_scripts/ folder test for up to 4 threads on the slow nodes; make sure --cpus-per-task=4 and --partition=slow is set in your slurm job.
$ ls /scratch/assignment1/test_scripts/*tester.pyc
pi_calculation_tester.pyc triangle_counting_tester.pyc page_rank_lock_tester.pyc page_rank_atomic_tester.pyc
Certain programs operate on graph datasets. Sample input graphs are available at /scratch/input_graphs/ on the compute nodes (note they are present on the compute nodes only, and hence you can access them via slurm only).
$ ls /scratch/input_graphs/*.cs*
lj.csc  lj.csr  roadNet-CA.csc  roadNet-CA.csr  test_25M_50M.csc  test_25M_50M.csr
If you are interested in checking out the original graph datasets (this is not required to solve the assignment), you can find them here.
If you'd like to test your solution with more graph datasets, you can create your own simple graphs as follows:
Create a file called testGraph.txt with the list of edges (one edge on each line in "<source> <destination>" form) in the graph. For example,
1 2
2 3
Run /scratch/input_graphs/SNAPtoBinary testGraph.txt testGraphConverted. This will create testGraphConverted.csr and testGraphConverted.csc files which are CSR and CSC representations of the graph.
To use the graphs in your solutions, use the command line argument --inputFile "testGraphConverted".
We have compiled a list of common mistakes and poor parallel programming practices that should be avoided. You are expected to understand them and avoid such mistakes.
1. Monte Carlo Pi Estimation [20 Points]
The value of Pi (3.14159) can be estimated using Monte Carlo method as described below:

Consider a circle of radius 0.5 units that is inscribed in a unit square (each side is 1 unit).
The ratio of their areas is: pi * r * r / 2r * 2r = pi / 4.
We randomly generate n points inside the square. Let c out of the n points fall inside the circle.
Pi is then approximated as: pi / 4 = c / n    ==>    pi = 4 * c / n.
The program below implements the above algorithm.

    uint circle_count = 0;
    double x_coord, y_coord;
    for (uint i = 0; i < n; i++) {
        x_coord = (2.0 * get_random_coordinate(&random_seed)) - 1.0;
        y_coord = (2.0 * get_random_coordinate(&random_seed)) - 1.0;
        if ((sqr(x_coord) + sqr(y_coord)) <= 1.0)
            circle_count++;
    }
    double pi_value = 4.0 * (double)circle_points / (double)n;
Our goal is to parallelize the above algorithm. Specifically, we are interested in parallelizing the for loop such that each thread generates (approximately) n/T points, where T is the number of threads. Below is the pseudo-code showing the logic of our parallel solution:

    Create T threads
    for each thread in parallel {
        Get the circle_count for (approximately) n/T points
    }
    total_circle_points = Accumulate the circle_counts from all threads
    pi_value = 4.0 * total_circle_points / n;
The serial implementation is available in pi_calculation.cpp. You have to parallelize the given serial implementation using C++11 std::thread.

Your parallel solution must satisfy the following:

The file should be named pi_calculation_parallel.cpp.
A different random seed must be used for each thread to ensure different points are generated across different threads.
Your parallel solution must output the following information:
Total number of threads used.
For each thread: the number of random points generated, the number of points within the circle, and the time taken to generate and process these points (your threads should be numbered between [0, T)).
The total number of points generated.
The total number of points within the circle.
The total time taken for the entire execution (the code region to be timed is highlighted using comments in the serial code).
The sample output can be found in sample_outputs/pi_calculation.output.
Please note that the output format should strictly match the expected format (including "spaces" and "commas"). You can test your code using the test script as follows:

    $ python /scratch/assignment1/test_scripts/pi_calculation_tester.pyc --execPath=<absolute path of pi_calculation_parallel>
2. Triangle Counting [15 Points]
The number of triangles in a graph can be computed by counting the number of triangles formed by each edge in the graph. For an edge (u, v), the number of triangles it forms is given by: |A ∩ B| where A is the set of inNeighbors of u excluding v, and B is the set of outNeighbors of v excluding u.

The program below counts the number of triangles in a given graph (count_triangles_between(u, v) computes the above formula).

    triangle_count = 0
    for u in Graph.all_vertices {
        for v in u.outNeighbors() {
            triangle_count += count_triangles_between(u, v)
        }
    }
    triangle_count = triangle_count / 3 // divide by 3 to get the number of unique triangles
Our goal is to parallelize the above algorithm such that each thread works on a sub-graph. Below is the pseudo-code showing the logic of our parallel solution:

    Create T threads
    for each thread in parallel {
        Compute the number of triangles created by the vertices allocated to the thread        
    }
    triangle_count = Accumulate the triangle counts from all the threads
    triangle_count = triangle_count / 3
The serial implementation is available in triangle_counting.cpp. You have to parallelize the given serial implementation using C++11 threads.

The file should be named triangle_counting_parallel.cpp. The input graph file should be specified using the command-line parameter --inputFile (similar to the serial code).
Your parallel solution must output the following information:
Total number of threads used.
For each thread: the number of triangles counted and the time taken to count the triangles (your threads should be numbered between [0, T)).
The total number of triangles in the graph.
The total number of unique triangles in the graph.
The total time taken for the entire execution (the code region to be timed is highlighted using comments in the serial code).
The sample console output can be found in sample_outputs/pi_calculation.output.
Please note that the output format should strictly match the expected format (including "spaces" and "commas"). You can test your code using the test script as follows:

    $ python /scratch/assignment1/test_scripts/triangle_counting_tester.pyc --execPath=<absolute path of triangle_counting_parallel>
3. PageRank with Locks [25 Points]
Given a graph, the PageRank of a vertex v is computed as:

pagerank[v] = (1 - DAMPING) + (DAMPING * Σ(pagerank[u] / out_degree[u]))
where DAMPING is a constant set to 0.85, and Σ represents summation over all incoming edges (u, v) of v.

Note that v might be an incoming neighbor for some other vertex w, and hence, we cannot simply overwrite pagerank[v] before pagerank[w] gets computed. To do so, we separate the pagerank values on the left of the equation with the pagerank values on the right of the equation, as shown here:

pagerank_next[v] = (1 - DAMPING) + (DAMPING * Σ (pagerank_curr[u] / out_degree[u]))
where pagerank_next[v] is the new pagerank of v and pagerank_curr[u] is the old pagerank of u.

The pagerank for all the vertices in the graph is computed multiple times for certain number of iterations. In this problem, we will look at an implementation of pagerank where each vertex u pushes its pagerank value to its outgoing neighbors, after which, the cumulative value received by each vertex gets used to compute pagerank of the vertex. The serial code shown below repeats this process for max_iters iterations.

   // Initialization
   for (uintV i = 0; i < n; i++) {
       pr_curr[i] = 1.0;
       pr_next[i] = 0.0;
   }

   // ----------------------------------------------------------------
   for (int iter = 0; iter < max_iters; iter++) {
       for (uintV u = 0; u < n; u++) {
           uintE out_degree = g.vertices_[u].getOutDegree();
           for (uintE i = 0; i < out_degree; i++) {
               uintV v = g.vertices_[u].getOutNeighbor(i);
               pr_next[v] += (pr_curr[u] / out_degree);
           }
       }
       for (uintV u = 0; u < n; u++) {
           // pr_next[u] contains the cumulative value received by u          
           pr_next[u] = (1 - DAMPING) + (DAMPING * pr_next[u]);  
           // Update pr_curr and reset pr_next for the next iteration
           pr_curr[u] = pr_next[u];
           pr_next[u] = 0.0;
       }
   }
   // ----------------------------------------------------------------
   
Our goal is to parallelize the above algorithm. Specifically, we are interested in parallelizing the for loop demarcated by comments (// ---) such that each thread works on a sub-graph. Below is the pseudo-code showing the logic of our parallel solution:

    Create T threads
    for(i=0; i<max_iterations; i++) {
        for each thread in parallel {
            for each vertex 'u' allocated to the thread {
                for vertex 'v' in outNeighbor(u)
                    next_page_rank[v] += (current_page_rank[u]/out_degree[u]) 
            }
        }
            
        for each thread in parallel {
            for each vertex 'v' allocated to the thread {
                compute the new_pagerank using the accumulated values in next_page_rank[v].
                current_page_rank[v] = new_pagerank
                Reset next_page_rank[v] to 0
            }
        }
    }
Key things to note:

Observe the variables shared by multiple threads. Ensure that access to shared resources is properly synchronized using locks. Use std::mutex as needed.
Observe the separation between the two for loops. Only after all the threads have processed the first for loop, the second for loop should be processed by each thread. You will need a barrier to achieve this. We have provided CustomBarrier in the utils.h which you can use as shown below:
CustomBarrier my_barrier(4);   //Create a barrier object. 4 --> number of workers/threads
// Share this my_barrier object with all the threads
// ---
    // Inside the thread function, wait on the barrier as follows:
    my_barrier.wait();
// ---
You must follow the logic outlined in the pseudo-code. For example, do not rewrite the loops differently to optimize the PageRank computation. Failure to follow the outlined approach will result in loss of points.
The serial implementation is available in page_rank.cpp. You have to parallelize the given serial implementation using C++11 std::thread and locks using std::mutex.

The output of your program can be verified by comparing the sum of all the pagerank values with that generated by the serial implementation. It is important to note that floating point multiplication is not associative. So, there may be minor variation in the pagerank values compared to serial implementation. For quick verification, we have also provided an integer version of the program. To run the integer version of PageRank, use the flag USE_INT=1 during make as follows:

    $ make USE_INT=1 page_rank
Note that the floating point based implementation is the default implementation (i.e., doesn't require any flags).

Your parallel solution must satisfy the following:

The file should be named page_rank_parallel.cpp and should support the following command-line parameters:
--nWorkers: The number of worker threads.
--nIterations: The number of iterations (similar to the serial code).
--inputFile: The absolute path to the input graph file (similar to the serial code).
Your parallel solution must output the following information:
Total number of threads used.
For each thread: the time taken to compute pageranks (your threads should be numbered between [0, T)).
The sum of pageranks of all vertices.
The total time taken for the entire execution (the code region to be timed is highlighted using comments in the serial code).
The sample console output can be found at sample_outputs/page_rank.output.
Please note that the output format should strictly match the expected format (including "spaces" and "commas"). You can test your code using the test script as follows (remember to run this via slurm):

    $ python /scratch/assignment1/test_scripts/page_rank_lock_tester.pyc --execPath=<absolute path of page_rank_parallel>
4. PageRank with Atomics [20 Points]
Here, we will improve the above PageRank solution by eliminating std::mutex locks. Recall that atomic operations like Compare-and-Swap perform two memory operations (read and write) atomically. Such atomic operations allow us to eliminate locks in our PageRank solution. C++11 supports std::atomics from which you should use compare_exchange. You need to figure out how to use compare_exchange correctly such that your parallel solution becomes lockless.

Your parallel solution must satisfy the following:

The file should be named page_rank_parallel_atomic.cpp and should support the following command-line parameters:
--nWorkers: The number of worker threads.
--nIterations: The number of iterations (similar to the serial code).
--inputFile: The absolute path to the input graph file (similar to the serial code).
Your parallel solution must output the following information:
Total number of threads used.
For each thread: the time taken to compute pageranks (your threads should be numbered between [0, T)).
The sum of pageranks of all vertices.
The total time taken for the entire execution (the code region to be timed is highlighted using comments in the serial code).
The sample console output can be found at sample_outputs/page_rank.output.
Please note that the output format should strictly match the expected format (including "spaces" and "commas"). You can test your code using the test script as follows (remember to run this via slurm):

    $ python /scratch/assignment1/test_scripts/page_rank_atomic_tester.pyc --execPath=<absolute path of page_rank_parallel_atomic>
Submission Guidelines
Make sure that your solutions folder has the following files and sub-folders. Let's say your solutions folder is called my_assignment1_solutions. It should contain:

core/ -- The folder containing all core files. It is already available in the assignment package. Do not modify it or remove any files.
Makefile -- Makefile for the project. This file should not be changed.
pi_calculation_parallel.cpp
triangle_counting_parallel.cpp
page_rank_parallel.cpp
page_rank_parallel_atomic.cpp
To create the submission file, follow the steps below:

Enter in your solutions folder, and remove all the object/temporary files.
$ cd my_assignment1_solutions/
$ make clean
Create the tar.gz file.
$ tar cvzf assignment1.tar.gz *
which creates a compressed tar ball that contains the contents of the folder.
Validate the tar ball using the submission_validator.pyc script.
$ python /scratch/assignment1/test_scripts/submission_validator.pyc --tarPath=assignment1.tar.gz
Submit via CourSys by the deadline posted there.

Copyright © 2023 Keval Vora. All rights reserved.